{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, special=[], min_freq=0, max_size=None,\n",
    "                 lower_case=True, delimiter=None, vocab_file=None):\n",
    "        self.counter = Counter()\n",
    "        self.special = special\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.lower_case = lower_case\n",
    "        self.delimiter = delimiter\n",
    "        self.vocab_file = vocab_file\n",
    "\n",
    "    def tokenize(self, line, add_eos=False, add_double_eos=False):\n",
    "        line = line.strip()\n",
    "        if self.lower_case:\n",
    "            line = line.lower()\n",
    "\n",
    "        if self.delimiter == \"\":\n",
    "            symbols = line\n",
    "        else:\n",
    "            # If None splits by space \n",
    "            symbols = line.split(self.delimiter)\n",
    "\n",
    "        if add_double_eos:  # lm1b\n",
    "            return ['<S>'] + symbols + ['<S>']\n",
    "        elif add_eos:\n",
    "            return symbols + ['<eos>']\n",
    "        else:\n",
    "            return symbols\n",
    "\n",
    "    def count_file(self, path, verbose=False, add_eos=False):\n",
    "        if verbose: print('counting file {} ...'.format(path))\n",
    "        assert tf.io.gfile.exists(path)\n",
    "\n",
    "        sents = []\n",
    "        with open(path, 'r') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                    print(' line {}'.format(idx))\n",
    "                # Split each sentence into token and increment\n",
    "                # counter for each token\n",
    "                symbols = self.tokenize(line, add_eos=add_eos)\n",
    "                self.counter.update(symbols)\n",
    "                sents.append(symbols)\n",
    "\n",
    "        return sents\n",
    "\n",
    "    def count_sents(self, sents, verbose=False):\n",
    "        if verbose: print('counting {} sents ...'.format(len(sents)))\n",
    "        for idx, symbols in enumerate(sents):\n",
    "            if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                print(' line {}'.format(idx))\n",
    "            self.counter.update(symbols)\n",
    "\n",
    "    def _build_from_file(self, vocab_file):\n",
    "        self.idx2sym = []\n",
    "        self.sym2idx = OrderedDict()\n",
    "\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            for line in f:\n",
    "                symb = line.strip().split()[0]\n",
    "                self.add_symbol(symb)\n",
    "        self.unk_idx = self.sym2idx['<UNK>']\n",
    "\n",
    "\n",
    "    def build_vocab(self):\n",
    "        if self.vocab_file:\n",
    "            print('building vocab from {}'.format(self.vocab_file))\n",
    "            self._build_from_file(self.vocab_file)\n",
    "            print('final vocab size {}'.format(len(self)))\n",
    "\n",
    "        else:\n",
    "            print('building vocab with min_freq={}, max_size={}'.format(\n",
    "                self.min_freq, self.max_size\n",
    "            ))\n",
    "            self.idx2sym = []\n",
    "            self.sym2idx = OrderedDict()\n",
    "\n",
    "            for sym in self.special:\n",
    "                self.add_special(sym)\n",
    "\n",
    "            for sym, cnt in self.counter.most_common(self.max_size):\n",
    "                # Works because words in most_common are\n",
    "                # in descending order of frequency so you\n",
    "                # can just stop if count is below the min value\n",
    "                if cnt < self.min_freq: break\n",
    "                self.add_symbol(sym)\n",
    "\n",
    "            print('final vocab size {} from {} unique tokens'.format(\n",
    "                len(self), len(self.counter))\n",
    "            )\n",
    "\n",
    "    def encode_file(self, path, ordered=False, verbose=False, add_eos=True, add_double_eos=False):\n",
    "\n",
    "        if verbose: print('encoding file {} ...'.format(path))\n",
    "        assert tf.io.gfile.exists(path)\n",
    "        encoded = []\n",
    "        with open(path, 'r') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                    print('  line {}'.format(idx))\n",
    "                symbols = self.tokenize(line, add_eos=add_eos, add_double_eos=add_double_eos)\n",
    "                encoded.append(self.convert_to_nparray(symbols))\n",
    "\n",
    "        if ordered:\n",
    "            encoded = np.concatenate(encoded)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode_sents(self, sents, ordered=False, verbose=False):\n",
    "        if verbose: print('encoding {} sents ...'.format(len(sents)))\n",
    "        encoded = []\n",
    "        for idx, symbols in enumerate(sents):\n",
    "            if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                print(' line {}'.format(idx))\n",
    "            encoded.append(self.convert_to_nparray(symbols))\n",
    "\n",
    "        if ordered:\n",
    "            encoded = np.concatenate(encoded)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def add_special(self, sym):\n",
    "        # Essentially same as add_symbol\n",
    "        # but also makes the symbol\n",
    "        # an attribute\n",
    "        if sym not in self.sym2idx:\n",
    "            self.idx2sym.append(sym)\n",
    "            self.sym2idx[sym] = len(self.idx2sym) - 1\n",
    "            setattr(self, '{}_idx'.format(sym.strip('<>')), self.sym2idx[sym])\n",
    "\n",
    "    def add_symbol(self, sym):\n",
    "        if sym not in self.sym2idx:\n",
    "            self.idx2sym.append(sym)\n",
    "            self.sym2idx[sym] = len(self.idx2sym) - 1\n",
    "\n",
    "    def get_sym(self, idx) -> str:\n",
    "        assert 0 <= idx < len(self), 'Index {} out of range'\n",
    "        return self.idx2sym[idx]\n",
    "\n",
    "    def get_idx(self, sym):\n",
    "        if sym in self.sym2idx:\n",
    "            return self.sym2idx[sym]\n",
    "        else:\n",
    "            hasattr(self, 'unk_idx')\n",
    "            return self.sym2idx.get(sym, self.unk_idx)\n",
    "\n",
    "    def get_symbols(self, indices):\n",
    "        return [self.get_sym(idx) for idx in indices]\n",
    "\n",
    "    def get_indices(self, symbols):\n",
    "        return [self.get_idx(sym) for sym in symbols]\n",
    "\n",
    "    def convert_to_nparray(self, symbols):\n",
    "        nparray = np.array(\n",
    "            self.get_indices(symbols), dtype=np.int64\n",
    "        )\n",
    "        return nparray\n",
    "\n",
    "    def convert_to_sent(self, indices, exclude=None):\n",
    "        if exclude is None:\n",
    "            return ' '.join([self.get_sym(idx) for idx in indices])\n",
    "        else:\n",
    "            return ' '.join([self.get_sym(idx) for idx in indices if idx not in exclude])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict()\n",
    "# I am using settings given for WikiText-103\n",
    "kwargs[\"special\"] = [\"<eos>\"]\n",
    "kwargs[\"lower_case\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vocab.count_file('train_pride_and_prejudice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab with min_freq=0, max_size=None\n",
      "final vocab size 6702 from 6701 unique tokens\n"
     ]
    }
   ],
   "source": [
    "vocab.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3venv",
   "language": "python",
   "name": "py3venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
